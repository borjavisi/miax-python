{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.2 Web Scraping I."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Web-Scrapping es la forma que tenemos para referirnos a la captura de información de cualquier sitio web. Su objetivo es capturar información de forma automática.\n",
    "\n",
    "Las librerías principales que vamos a utilizar son beautifulsoup y requests. Para ello, hay que instalarlas primero:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez instalado, se importa en el código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El proceso se divide principalmente en tres fases:\n",
    "\n",
    "- Carga de la dirección web a la que realizar el scrapping. A través de  requests.\n",
    "\n",
    "- Extracción del contenido de la web a partir de Beautifulsoup\n",
    "\n",
    "- Manipulación del contenido.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga de la URL\n",
    "\n",
    "Request necesita dos cosas: La URL y un elemento cabecera. El elemento cabecera es vital, ya que muchas web no permiten el acceso a la información a no ser que detecten que las peticiones vienen de un navegador. El más utilizado para este propósito es Mozilla Firefox.\n",
    "\n",
    "Así, definimos headers como un diccionario cuyo User-Agent es Firefox:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Firefox'}\n",
    "url = 'https://resultados.as.com/resultados/futbol/primera/2015_2016/jornada/regular_a_1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re = requests.get(url,headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora debemos preguntar si la conexion con esa pagina web ha funcionado. Es decir si la propiedad status_code que devuelve re es 200 o 201. Si es así, pasamos a la siguiente fase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(re.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En re.content tendremos todo el HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "re.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Extracción del contenido con BeautifulSoup\n",
    "\n",
    "Lo primero que tenemos que hacer es pasar al contenido que se encuentra en re.text a través de un parser de HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup( re.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo siguiente que tenemos que hacer es identificar qué información es la que nos queremos descargar, y dónde se encuentra dentro del HTML\n",
    "\n",
    "Todos los elementos de una página tienen un xpath que es unívoco a ese elemento, por lo que puedes usarlo para recuperar la información\n",
    "\n",
    "- Inspeccionar, botón derecho sobre el elemento, copiar, copiar XPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"./imgs/xpath.png\"  alt=\"drawing\" width=\"600\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el ejemplo, vamos a descargarnos la información de la jonada de fútbol de LaLiga Santander\n",
    "\n",
    "<center>\n",
    "<img src=\"./imgs/info_a_descargar.png\"  alt=\"drawing\" width=\"600\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada uno de los resultados tienen la etiqueta li, y la clase list-resultado. Por lo que localizamos todos los elementos que cumplan estas condiciones y los guardamos\n",
    "\n",
    "<center>\n",
    "<img src=\"./imgs/ejemplo_1.png\"  alt=\"drawing\" width=\"600\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = soup.find_all('li',{'class' : 'list-resultado'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya los tenemos todos. Vamos a ver qué pinta tienen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No es precisamente información limpia. \n",
    "\n",
    "Tendremos que iterar por los artículos para obtener la información que nos interesa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in articles:\n",
    "    \n",
    "    equipos = article.find_all('span',{'class' : 'nombre-equipo'})\n",
    "    \n",
    "    for equipo in equipos:\n",
    "        \n",
    "        eq =equipo.text\n",
    "        print(eq)\n",
    "    \n",
    "    resultado = article.find('div',{'class' : 'cont-resultado finalizado'}).getText()\n",
    "    print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos acceder a cada elemento con el nombre del tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "soup.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si somos específicos, obtendremos mejores resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.head.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con .text o .string accedemos al contenido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.head.title.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Manipulación del contenido\n",
    "\n",
    "En esta fase vamos iterando sobre lo que hemos conseguido con BeautifulSoup.\n",
    "\n",
    "Si el contenido es directamente la información que se encuentra en lo encontrado por soup.find.all, sólo hay que imprimir la información, o guardarla en un DF.\n",
    "\n",
    "Sin embargo a veces hay que volver a llamar volver a repetir la búsqueda con find, itereando sobre la información descargada y filtrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in articles:\n",
    "\n",
    "    masinfo= article.find_all('div',{'class' : 'info-evento'})\n",
    "\n",
    "    for info in masinfo:\n",
    "        fechahora = info.find('time').get('content')\n",
    "        print(fechahora)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el anterior código, vemos que para obtener la fecha y la hora simplemente utilizamos find. Este es el caso cuando no hay una lista de elementos. Suele devolver un dato ya disponible, no un objeto iterable.\n",
    "\n",
    "Otra alternativa que podemos probar cuando este get no funcione porque necesitemos especificar la clase es:\n",
    "\n",
    "    resultado = enlace.find('a',{'class' : 'resultado'}).getText()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Conclusión\n",
    "\n",
    "El proceso de Web-Scrapping no es un proceso complicado, pero si tedioso.\n",
    "Y es tedioso porque hay que comprender cuál es la estructura de la web que queremos scrappear y es posible que con el tiempo, un web-scrapper que funcionase, no nos funcione actualmente por qu hayan cambiado la estructura de la web.\n",
    "\n",
    "Para asentar conocimientos, vamos a probar a extraer la misma información que ya obtuvimos haciendo este mismo proceso en R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"./imgs/wikipedia.png\"  alt=\"drawing\" width=\"600\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los primeros pasos son exáctamente iguales\n",
    "\n",
    "Lo único que tenemos que adaptar es la url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Firefox'}\n",
    "url = 'https://es.wikipedia.org/wiki/El_lobo_de_Wall_Street'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re = requests.get(url,headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup( re.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capturamos la ficha técnica de la película (la tabla que está a la derecha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_a_extraer = soup.find_all('table',{'class' : 'infobox plainlist plainlinks'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es la primera tabla, por lo que debemos indicar que nos quedamos con únicamente esa tabla\n",
    "\n",
    "Las filas de la tabla tienen la etiqueta tr, por lo que localizamos todos los elementos que tengan dicha etiqueta, para iterar sobre ellos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_a_extraer = datos_a_extraer[0]\n",
    "trs = datos_a_extraer.find_all('tr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al iterar sobre las filas, queremos extraer los elementos de las columnas de la izquierda th y de la derecha td\n",
    "\n",
    "Extraemos todos e iteramos sobre ellos para extraer la información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_th = []\n",
    "rows_td = []\n",
    "\n",
    "for tr in trs:\n",
    "    \n",
    "    ths = tr.find_all('th')\n",
    "    tds = tr.find_all('td')\n",
    "    \n",
    "    for th in ths:\n",
    "                        \n",
    "        texto = th.text\n",
    "        # print(th.text)\n",
    "        rows_th.append(texto)\n",
    "        \n",
    "    for td in tds:\n",
    "                \n",
    "        texto = td.text\n",
    "        #print(td.text)\n",
    "        rows_td.append(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado = pd.DataFrame(np.column_stack([rows_th, rows_td]))\n",
    "resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, si analizamos el resultado obtenido, no es exáctamente lo que queremos.\n",
    "\n",
    "La información está descolocada. Y eso es porque hay varios elementos en la tabla que solo están en una de las dos columnas.\n",
    "\n",
    "Tenemos que identificarlos, y evitar descargarlos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"./imgs/tabla_mal.png\"  alt=\"drawing\" width=\"600\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_th = []\n",
    "rows_td = []\n",
    "\n",
    "for tr in trs:\n",
    "    \n",
    "    ths = tr.find_all('th')\n",
    "    tds = tr.find_all('td')\n",
    "    \n",
    "    for th in ths:\n",
    "        \n",
    "        if \"Ficha técnica\" in str(th) or \"Datos y cifras\" in str(th) or \"Compañías\" in str(th):\n",
    "            continue\n",
    "        \n",
    "        texto = th.text\n",
    "        # print(th.text)\n",
    "        rows_th.append(texto)\n",
    "        \n",
    "    for td in tds:\n",
    "        \n",
    "        if \"Ver todos los créditos\" in str(td) or \"Ficha\" in str(td) or \"Wikidata\" in str(td):\n",
    "            continue\n",
    "        \n",
    "        texto = td.text\n",
    "        texto = texto.replace('\\n',\"\")\n",
    "        #print(td.text)\n",
    "        rows_td.append(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado = pd.DataFrame(np.column_stack([rows_th, rows_td]))\n",
    "resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora sí que lo hemos conseguido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capturamos ahora las referencias de la película\n",
    "\n",
    "Ya tenemos descargado el contenido en soup\n",
    "\n",
    "Lo que tenemos que hacer es localizar el elemento y clase por el que queremos filtrar\n",
    "\n",
    "En este caso, ol y references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_a_extraer = soup.find_all('ol',{'class' : 'references'})\n",
    "datos_a_extraer = datos_a_extraer[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que cada una de las referencias tiene un tag que se llama li. \n",
    "\n",
    "Localizamos todas e iteramos por ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lis = datos_a_extraer.find_all('li')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for info in lis:\n",
    "    texto = info.find('span',{'class' : 'reference-text'}).getText()    \n",
    "    # print(texto)\n",
    "    rows.append(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertimos la información en un dataframe y ya lo tenemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rows)\n",
    "df.columns = ['Referencias']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capturemos ahora el argumento de la película\n",
    "\n",
    "El elemento por el que queremos filtrar es div, y la clase mw-parser-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_a_extraer = soup.find_all('div',{'class' : 'mw-parser-output'})\n",
    "datos_a_extraer = datos_a_extraer[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada uno de los párrafos tienen la etiqueta p\n",
    "\n",
    "Localizamos todos e itereamos por ellos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis = datos_a_extraer.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for li in lis:\n",
    "    \n",
    "    texto = li.text\n",
    "    #print(texto)\n",
    "    rows.append(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rows)\n",
    "df.columns = ['Argumento']\n",
    "df = df.iloc[2:-3,:]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.2.1** Captura los hechos relevantes de la CNMV: http://www.cnmv.es/portal/HR/HRAldia.aspx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.2.2** Captura las noticias sobre economía de Expansion: http://www.expansion.com/economia.html?intcmp=MENUHOM24101&s_kw=economia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.2.3** Captura la agenda macroeconómica de investing.com: http://es.investing.com/economic-calendar/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.2.4** Captura las cotizaciones de los commodities de investing.com: https://es.investing.com/commodities/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.2.5** Captura las cotizaciones del Ibex 35 de Bolsa de Madrid: http://www.bolsamadrid.es/esp/aspx/Mercados/Precios.aspx?indice=ESI100000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.2.6** De la página de bolsa de madrid: https://www.bolsamadrid.es/esp/aspx/Indices/Resumen.aspx obten un daframe con los contenidos de la tabla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.2.7** Obtén las últimas noticias (expansión) y precios objetivo (expansión) de: Telefónica, Santander y Siemens Gamesa\n",
    "\n",
    "Noticias: \n",
    "\n",
    "- https://www.expansion.com/mercados/cotizaciones/valores/telefonica_M.TEF.html\n",
    "- https://www.expansion.com/mercados/cotizaciones/valores/santander_M.SAN.html\n",
    "- https://www.expansion.com/mercados/cotizaciones/valores/siemensgamesa_M.SGRE.html\n",
    "\n",
    "Precios objetivo: \n",
    "\n",
    "- https://www.expansion.com/mercados/bolsa/recomendaciones/consenso-mercados/t/telefonica_M.TEF.html\n",
    "- https://www.expansion.com/mercados/bolsa/recomendaciones/consenso-mercados/s/santander_M.SAN.html\n",
    "- https://www.expansion.com/mercados/bolsa/recomendaciones/consenso-mercados/s/siemensgamesa_M.SGRE.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.2.8** Desarrolla una función que sea capaz de capturar todas las noticias que tengan relación con una cualquier empresa y que hayan tenido lugar en un intervalo determinado. Utiliza para ello la hemeroteca de ABC. Pueba a obtener todas la noticias de Telefónica entre el 1 de enero y 31 de enero de 2016.\n",
    "\n",
    "- http://www.abc.es/hemeroteca/resultados-busqueda-avanzada/noticia/pagina-1?exa=telefonica&rfec=20160101;20161031&or=1&nres=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
