{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios Día 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.arange(0,100)\n",
    "y = x*2\n",
    "z = x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Visualización de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Exercise 1\n",
    "Follow along with these steps:\n",
    "\n",
    "Create a figure object called fig using plt.figure()\n",
    "Use add_axes to add an axis to the figure canvas at [0,0,1,1]. Call this new axis ax.\n",
    "Plot (x,y) on that axes and set the labels and titles to match the plot below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Exercise 4\n",
    "Use plt.subplots(nrows=1, ncols=2) \n",
    "Now plot (x,y) and (x,z) on the axes. Play around with the linewidth and style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "See if you can resize the plot by adding the figsize() argument in plt.subplots() are copying and pasting your previous code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Adquisición y guardado de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Limpieza y preparación de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Operaciones de combinar, juntar y agrupar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Tratamiento de series temporales I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Tratamiento de series temporales II."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'infer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmangle_dupe_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfalse_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipinitialspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipfooter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_default_na\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_filter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_blank_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfer_datetime_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_date_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_parser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdayfirst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'infer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthousands\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mb'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineterminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquotechar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\"'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquoting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoublequote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mescapechar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdialect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtupleize_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_bad_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn_bad_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelim_whitespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat_precision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Read a comma-separated values (csv) file into DataFrame.\n",
       "\n",
       "Also supports optionally iterating or breaking of the file\n",
       "into chunks.\n",
       "\n",
       "Additional help can be found in the online docs for\n",
       "`IO Tools <http://pandas.pydata.org/pandas-docs/stable/io.html>`_.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "filepath_or_buffer : str, path object, or file-like object\n",
       "    Any valid string path is acceptable. The string could be a URL. Valid\n",
       "    URL schemes include http, ftp, s3, and file. For file URLs, a host is\n",
       "    expected. A local file could be: file://localhost/path/to/table.csv.\n",
       "\n",
       "    If you want to pass in a path object, pandas accepts either\n",
       "    ``pathlib.Path`` or ``py._path.local.LocalPath``.\n",
       "\n",
       "    By file-like object, we refer to objects with a ``read()`` method, such as\n",
       "    a file handler (e.g. via builtin ``open`` function) or ``StringIO``.\n",
       "sep : str, default ','\n",
       "    Delimiter to use. If sep is None, the C engine cannot automatically detect\n",
       "    the separator, but the Python parsing engine can, meaning the latter will\n",
       "    be used and automatically detect the separator by Python's builtin sniffer\n",
       "    tool, ``csv.Sniffer``. In addition, separators longer than 1 character and\n",
       "    different from ``'\\s+'`` will be interpreted as regular expressions and\n",
       "    will also force the use of the Python parsing engine. Note that regex\n",
       "    delimiters are prone to ignoring quoted data. Regex example: ``'\\r\\t'``.\n",
       "delimiter : str, default ``None``\n",
       "    Alias for sep.\n",
       "header : int, list of int, default 'infer'\n",
       "    Row number(s) to use as the column names, and the start of the\n",
       "    data.  Default behavior is to infer the column names: if no names\n",
       "    are passed the behavior is identical to ``header=0`` and column\n",
       "    names are inferred from the first line of the file, if column\n",
       "    names are passed explicitly then the behavior is identical to\n",
       "    ``header=None``. Explicitly pass ``header=0`` to be able to\n",
       "    replace existing names. The header can be a list of integers that\n",
       "    specify row locations for a multi-index on the columns\n",
       "    e.g. [0,1,3]. Intervening rows that are not specified will be\n",
       "    skipped (e.g. 2 in this example is skipped). Note that this\n",
       "    parameter ignores commented lines and empty lines if\n",
       "    ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n",
       "    data rather than the first line of the file.\n",
       "names : array-like, optional\n",
       "    List of column names to use. If file contains no header row, then you\n",
       "    should explicitly pass ``header=None``. Duplicates in this list will cause\n",
       "    a ``UserWarning`` to be issued.\n",
       "index_col : int, sequence or bool, optional\n",
       "    Column to use as the row labels of the DataFrame. If a sequence is given, a\n",
       "    MultiIndex is used. If you have a malformed file with delimiters at the end\n",
       "    of each line, you might consider ``index_col=False`` to force pandas to\n",
       "    not use the first column as the index (row names).\n",
       "usecols : list-like or callable, optional\n",
       "    Return a subset of the columns. If list-like, all elements must either\n",
       "    be positional (i.e. integer indices into the document columns) or strings\n",
       "    that correspond to column names provided either by the user in `names` or\n",
       "    inferred from the document header row(s). For example, a valid list-like\n",
       "    `usecols` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n",
       "    Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n",
       "    To instantiate a DataFrame from ``data`` with element order preserved use\n",
       "    ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]`` for columns\n",
       "    in ``['foo', 'bar']`` order or\n",
       "    ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n",
       "    for ``['bar', 'foo']`` order.\n",
       "\n",
       "    If callable, the callable function will be evaluated against the column\n",
       "    names, returning names where the callable function evaluates to True. An\n",
       "    example of a valid callable argument would be ``lambda x: x.upper() in\n",
       "    ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n",
       "    parsing time and lower memory usage.\n",
       "squeeze : bool, default False\n",
       "    If the parsed data only contains one column then return a Series.\n",
       "prefix : str, optional\n",
       "    Prefix to add to column numbers when no header, e.g. 'X' for X0, X1, ...\n",
       "mangle_dupe_cols : bool, default True\n",
       "    Duplicate columns will be specified as 'X', 'X.1', ...'X.N', rather than\n",
       "    'X'...'X'. Passing in False will cause data to be overwritten if there\n",
       "    are duplicate names in the columns.\n",
       "dtype : Type name or dict of column -> type, optional\n",
       "    Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32,\n",
       "    'c': 'Int64'}\n",
       "    Use `str` or `object` together with suitable `na_values` settings\n",
       "    to preserve and not interpret dtype.\n",
       "    If converters are specified, they will be applied INSTEAD\n",
       "    of dtype conversion.\n",
       "engine : {'c', 'python'}, optional\n",
       "    Parser engine to use. The C engine is faster while the python engine is\n",
       "    currently more feature-complete.\n",
       "converters : dict, optional\n",
       "    Dict of functions for converting values in certain columns. Keys can either\n",
       "    be integers or column labels.\n",
       "true_values : list, optional\n",
       "    Values to consider as True.\n",
       "false_values : list, optional\n",
       "    Values to consider as False.\n",
       "skipinitialspace : bool, default False\n",
       "    Skip spaces after delimiter.\n",
       "skiprows : list-like, int or callable, optional\n",
       "    Line numbers to skip (0-indexed) or number of lines to skip (int)\n",
       "    at the start of the file.\n",
       "\n",
       "    If callable, the callable function will be evaluated against the row\n",
       "    indices, returning True if the row should be skipped and False otherwise.\n",
       "    An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\n",
       "skipfooter : int, default 0\n",
       "    Number of lines at bottom of file to skip (Unsupported with engine='c').\n",
       "nrows : int, optional\n",
       "    Number of rows of file to read. Useful for reading pieces of large files.\n",
       "na_values : scalar, str, list-like, or dict, optional\n",
       "    Additional strings to recognize as NA/NaN. If dict passed, specific\n",
       "    per-column NA values.  By default the following values are interpreted as\n",
       "    NaN: '', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan',\n",
       "    '1.#IND', '1.#QNAN', 'N/A', 'NA', 'NULL', 'NaN', 'n/a', 'nan',\n",
       "    'null'.\n",
       "keep_default_na : bool, default True\n",
       "    Whether or not to include the default NaN values when parsing the data.\n",
       "    Depending on whether `na_values` is passed in, the behavior is as follows:\n",
       "\n",
       "    * If `keep_default_na` is True, and `na_values` are specified, `na_values`\n",
       "      is appended to the default NaN values used for parsing.\n",
       "    * If `keep_default_na` is True, and `na_values` are not specified, only\n",
       "      the default NaN values are used for parsing.\n",
       "    * If `keep_default_na` is False, and `na_values` are specified, only\n",
       "      the NaN values specified `na_values` are used for parsing.\n",
       "    * If `keep_default_na` is False, and `na_values` are not specified, no\n",
       "      strings will be parsed as NaN.\n",
       "\n",
       "    Note that if `na_filter` is passed in as False, the `keep_default_na` and\n",
       "    `na_values` parameters will be ignored.\n",
       "na_filter : bool, default True\n",
       "    Detect missing value markers (empty strings and the value of na_values). In\n",
       "    data without any NAs, passing na_filter=False can improve the performance\n",
       "    of reading a large file.\n",
       "verbose : bool, default False\n",
       "    Indicate number of NA values placed in non-numeric columns.\n",
       "skip_blank_lines : bool, default True\n",
       "    If True, skip over blank lines rather than interpreting as NaN values.\n",
       "parse_dates : bool or list of int or names or list of lists or dict, default False\n",
       "    The behavior is as follows:\n",
       "\n",
       "    * boolean. If True -> try parsing the index.\n",
       "    * list of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\n",
       "      each as a separate date column.\n",
       "    * list of lists. e.g.  If [[1, 3]] -> combine columns 1 and 3 and parse as\n",
       "      a single date column.\n",
       "    * dict, e.g. {'foo' : [1, 3]} -> parse columns 1, 3 as date and call\n",
       "      result 'foo'\n",
       "\n",
       "    If a column or index cannot be represented as an array of datetimes,\n",
       "    say because of an unparseable value or a mixture of timezones, the column\n",
       "    or index will be returned unaltered as an object data type. For\n",
       "    non-standard datetime parsing, use ``pd.to_datetime`` after\n",
       "    ``pd.read_csv``. To parse an index or column with a mixture of timezones,\n",
       "    specify ``date_parser`` to be a partially-applied\n",
       "    :func:`pandas.to_datetime` with ``utc=True``. See\n",
       "    :ref:`io.csv.mixed_timezones` for more.\n",
       "\n",
       "    Note: A fast-path exists for iso8601-formatted dates.\n",
       "infer_datetime_format : bool, default False\n",
       "    If True and `parse_dates` is enabled, pandas will attempt to infer the\n",
       "    format of the datetime strings in the columns, and if it can be inferred,\n",
       "    switch to a faster method of parsing them. In some cases this can increase\n",
       "    the parsing speed by 5-10x.\n",
       "keep_date_col : bool, default False\n",
       "    If True and `parse_dates` specifies combining multiple columns then\n",
       "    keep the original columns.\n",
       "date_parser : function, optional\n",
       "    Function to use for converting a sequence of string columns to an array of\n",
       "    datetime instances. The default uses ``dateutil.parser.parser`` to do the\n",
       "    conversion. Pandas will try to call `date_parser` in three different ways,\n",
       "    advancing to the next if an exception occurs: 1) Pass one or more arrays\n",
       "    (as defined by `parse_dates`) as arguments; 2) concatenate (row-wise) the\n",
       "    string values from the columns defined by `parse_dates` into a single array\n",
       "    and pass that; and 3) call `date_parser` once for each row using one or\n",
       "    more strings (corresponding to the columns defined by `parse_dates`) as\n",
       "    arguments.\n",
       "dayfirst : bool, default False\n",
       "    DD/MM format dates, international and European format.\n",
       "iterator : bool, default False\n",
       "    Return TextFileReader object for iteration or getting chunks with\n",
       "    ``get_chunk()``.\n",
       "chunksize : int, optional\n",
       "    Return TextFileReader object for iteration.\n",
       "    See the `IO Tools docs\n",
       "    <http://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n",
       "    for more information on ``iterator`` and ``chunksize``.\n",
       "compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n",
       "    For on-the-fly decompression of on-disk data. If 'infer' and\n",
       "    `filepath_or_buffer` is path-like, then detect compression from the\n",
       "    following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no\n",
       "    decompression). If using 'zip', the ZIP file must contain only one data\n",
       "    file to be read in. Set to None for no decompression.\n",
       "\n",
       "    .. versionadded:: 0.18.1 support for 'zip' and 'xz' compression.\n",
       "\n",
       "thousands : str, optional\n",
       "    Thousands separator.\n",
       "decimal : str, default '.'\n",
       "    Character to recognize as decimal point (e.g. use ',' for European data).\n",
       "lineterminator : str (length 1), optional\n",
       "    Character to break file into lines. Only valid with C parser.\n",
       "quotechar : str (length 1), optional\n",
       "    The character used to denote the start and end of a quoted item. Quoted\n",
       "    items can include the delimiter and it will be ignored.\n",
       "quoting : int or csv.QUOTE_* instance, default 0\n",
       "    Control field quoting behavior per ``csv.QUOTE_*`` constants. Use one of\n",
       "    QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).\n",
       "doublequote : bool, default ``True``\n",
       "   When quotechar is specified and quoting is not ``QUOTE_NONE``, indicate\n",
       "   whether or not to interpret two consecutive quotechar elements INSIDE a\n",
       "   field as a single ``quotechar`` element.\n",
       "escapechar : str (length 1), optional\n",
       "    One-character string used to escape other characters.\n",
       "comment : str, optional\n",
       "    Indicates remainder of line should not be parsed. If found at the beginning\n",
       "    of a line, the line will be ignored altogether. This parameter must be a\n",
       "    single character. Like empty lines (as long as ``skip_blank_lines=True``),\n",
       "    fully commented lines are ignored by the parameter `header` but not by\n",
       "    `skiprows`. For example, if ``comment='#'``, parsing\n",
       "    ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in 'a,b,c' being\n",
       "    treated as the header.\n",
       "encoding : str, optional\n",
       "    Encoding to use for UTF when reading/writing (ex. 'utf-8'). `List of Python\n",
       "    standard encodings\n",
       "    <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n",
       "dialect : str or csv.Dialect, optional\n",
       "    If provided, this parameter will override values (default or not) for the\n",
       "    following parameters: `delimiter`, `doublequote`, `escapechar`,\n",
       "    `skipinitialspace`, `quotechar`, and `quoting`. If it is necessary to\n",
       "    override values, a ParserWarning will be issued. See csv.Dialect\n",
       "    documentation for more details.\n",
       "tupleize_cols : bool, default False\n",
       "    Leave a list of tuples on columns as is (default is to convert to\n",
       "    a MultiIndex on the columns).\n",
       "\n",
       "    .. deprecated:: 0.21.0\n",
       "       This argument will be removed and will always convert to MultiIndex\n",
       "\n",
       "error_bad_lines : bool, default True\n",
       "    Lines with too many fields (e.g. a csv line with too many commas) will by\n",
       "    default cause an exception to be raised, and no DataFrame will be returned.\n",
       "    If False, then these \"bad lines\" will dropped from the DataFrame that is\n",
       "    returned.\n",
       "warn_bad_lines : bool, default True\n",
       "    If error_bad_lines is False, and warn_bad_lines is True, a warning for each\n",
       "    \"bad line\" will be output.\n",
       "delim_whitespace : bool, default False\n",
       "    Specifies whether or not whitespace (e.g. ``' '`` or ``'    '``) will be\n",
       "    used as the sep. Equivalent to setting ``sep='\\s+'``. If this option\n",
       "    is set to True, nothing should be passed in for the ``delimiter``\n",
       "    parameter.\n",
       "\n",
       "    .. versionadded:: 0.18.1 support for the Python parser.\n",
       "\n",
       "low_memory : bool, default True\n",
       "    Internally process the file in chunks, resulting in lower memory use\n",
       "    while parsing, but possibly mixed type inference.  To ensure no mixed\n",
       "    types either set False, or specify the type with the `dtype` parameter.\n",
       "    Note that the entire file is read into a single DataFrame regardless,\n",
       "    use the `chunksize` or `iterator` parameter to return the data in chunks.\n",
       "    (Only valid with C parser).\n",
       "memory_map : bool, default False\n",
       "    If a filepath is provided for `filepath_or_buffer`, map the file object\n",
       "    directly onto memory and access the data directly from there. Using this\n",
       "    option can improve performance because there is no longer any I/O overhead.\n",
       "float_precision : str, optional\n",
       "    Specifies which converter the C engine should use for floating-point\n",
       "    values. The options are `None` for the ordinary converter,\n",
       "    `high` for the high-precision converter, and `round_trip` for the\n",
       "    round-trip converter.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "DataFrame or TextParser\n",
       "    A comma-separated values (csv) file is returned as two-dimensional\n",
       "    data structure with labeled axes.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "to_csv : Write DataFrame to a comma-separated values (csv) file.\n",
       "read_csv : Read a comma-separated values (csv) file into DataFrame.\n",
       "read_fwf : Read a table of fixed-width formatted lines into DataFrame.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> pd.read_csv('data.csv')  # doctest: +SKIP\n",
       "\u001b[0;31mFile:\u001b[0m      ~/projects/python_venvs/py-dyn/lib/python3.6/site-packages/pandas/io/parsers.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?pd.read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibex = pd.read_csv('ibex.csv')\n",
    "ibex.index = pd.to_datetime(ibex['Unnamed: 0'])\n",
    "ibex.index.names = ['Date']\n",
    "ibex = ibex.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>vol</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-03</th>\n",
       "      <td>11610.0</td>\n",
       "      <td>11881.8</td>\n",
       "      <td>11574.4</td>\n",
       "      <td>11846.6</td>\n",
       "      <td>60107000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-04</th>\n",
       "      <td>11206.6</td>\n",
       "      <td>11530.0</td>\n",
       "      <td>11159.8</td>\n",
       "      <td>11499.5</td>\n",
       "      <td>62539000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-05</th>\n",
       "      <td>10863.1</td>\n",
       "      <td>11068.1</td>\n",
       "      <td>10824.9</td>\n",
       "      <td>11206.6</td>\n",
       "      <td>68153000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-07</th>\n",
       "      <td>11102.4</td>\n",
       "      <td>11137.9</td>\n",
       "      <td>10882.7</td>\n",
       "      <td>10963.4</td>\n",
       "      <td>144207000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-10</th>\n",
       "      <td>11173.3</td>\n",
       "      <td>11364.3</td>\n",
       "      <td>11120.6</td>\n",
       "      <td>11363.8</td>\n",
       "      <td>133817000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-11</th>\n",
       "      <td>11012.4</td>\n",
       "      <td>11234.6</td>\n",
       "      <td>10919.9</td>\n",
       "      <td>11233.0</td>\n",
       "      <td>73650000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-12</th>\n",
       "      <td>10851.8</td>\n",
       "      <td>10947.1</td>\n",
       "      <td>10821.4</td>\n",
       "      <td>10932.9</td>\n",
       "      <td>115797000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-13</th>\n",
       "      <td>10931.9</td>\n",
       "      <td>11169.8</td>\n",
       "      <td>10827.6</td>\n",
       "      <td>10989.9</td>\n",
       "      <td>139485000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-14</th>\n",
       "      <td>11184.0</td>\n",
       "      <td>11212.1</td>\n",
       "      <td>10976.3</td>\n",
       "      <td>10980.7</td>\n",
       "      <td>97119000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-17</th>\n",
       "      <td>11285.6</td>\n",
       "      <td>11394.2</td>\n",
       "      <td>11191.5</td>\n",
       "      <td>11384.9</td>\n",
       "      <td>94058000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-18</th>\n",
       "      <td>11066.3</td>\n",
       "      <td>11355.4</td>\n",
       "      <td>11032.5</td>\n",
       "      <td>11335.7</td>\n",
       "      <td>88682000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-19</th>\n",
       "      <td>11120.7</td>\n",
       "      <td>11134.4</td>\n",
       "      <td>10931.6</td>\n",
       "      <td>10981.5</td>\n",
       "      <td>79358000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-20</th>\n",
       "      <td>11047.6</td>\n",
       "      <td>11258.1</td>\n",
       "      <td>10970.4</td>\n",
       "      <td>11139.7</td>\n",
       "      <td>74005000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-21</th>\n",
       "      <td>10969.3</td>\n",
       "      <td>11073.9</td>\n",
       "      <td>10904.4</td>\n",
       "      <td>10992.2</td>\n",
       "      <td>80696000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-24</th>\n",
       "      <td>10955.0</td>\n",
       "      <td>11142.2</td>\n",
       "      <td>10908.8</td>\n",
       "      <td>11060.9</td>\n",
       "      <td>53854000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-25</th>\n",
       "      <td>10863.5</td>\n",
       "      <td>10901.5</td>\n",
       "      <td>10808.8</td>\n",
       "      <td>10828.2</td>\n",
       "      <td>65985000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-26</th>\n",
       "      <td>11031.4</td>\n",
       "      <td>11044.3</td>\n",
       "      <td>10923.6</td>\n",
       "      <td>10965.1</td>\n",
       "      <td>78070000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-27</th>\n",
       "      <td>11206.6</td>\n",
       "      <td>11293.6</td>\n",
       "      <td>11084.2</td>\n",
       "      <td>11134.3</td>\n",
       "      <td>118800000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-28</th>\n",
       "      <td>11009.3</td>\n",
       "      <td>11310.9</td>\n",
       "      <td>10987.5</td>\n",
       "      <td>11307.3</td>\n",
       "      <td>82913000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-31</th>\n",
       "      <td>10835.1</td>\n",
       "      <td>10960.0</td>\n",
       "      <td>10770.5</td>\n",
       "      <td>10889.1</td>\n",
       "      <td>97823000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-01</th>\n",
       "      <td>10970.0</td>\n",
       "      <td>10987.9</td>\n",
       "      <td>10840.0</td>\n",
       "      <td>10960.7</td>\n",
       "      <td>66505000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-02</th>\n",
       "      <td>11195.6</td>\n",
       "      <td>11215.8</td>\n",
       "      <td>10926.7</td>\n",
       "      <td>11049.7</td>\n",
       "      <td>70471000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-03</th>\n",
       "      <td>11544.1</td>\n",
       "      <td>11564.5</td>\n",
       "      <td>11208.9</td>\n",
       "      <td>11275.1</td>\n",
       "      <td>69657000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-04</th>\n",
       "      <td>11580.1</td>\n",
       "      <td>11704.9</td>\n",
       "      <td>11510.7</td>\n",
       "      <td>11592.9</td>\n",
       "      <td>65574000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-07</th>\n",
       "      <td>11506.8</td>\n",
       "      <td>11708.7</td>\n",
       "      <td>11469.0</td>\n",
       "      <td>11617.2</td>\n",
       "      <td>61544000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-08</th>\n",
       "      <td>11834.8</td>\n",
       "      <td>11834.8</td>\n",
       "      <td>11492.0</td>\n",
       "      <td>11567.1</td>\n",
       "      <td>70379000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-09</th>\n",
       "      <td>11939.5</td>\n",
       "      <td>12102.8</td>\n",
       "      <td>11851.1</td>\n",
       "      <td>11932.8</td>\n",
       "      <td>79532000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-10</th>\n",
       "      <td>12020.6</td>\n",
       "      <td>12067.6</td>\n",
       "      <td>11757.8</td>\n",
       "      <td>11766.3</td>\n",
       "      <td>70339000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-11</th>\n",
       "      <td>12432.6</td>\n",
       "      <td>12463.7</td>\n",
       "      <td>12133.8</td>\n",
       "      <td>12170.4</td>\n",
       "      <td>95879000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-14</th>\n",
       "      <td>12458.6</td>\n",
       "      <td>12912.3</td>\n",
       "      <td>12333.5</td>\n",
       "      <td>12595.5</td>\n",
       "      <td>132608000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-01</th>\n",
       "      <td>9341.7</td>\n",
       "      <td>9355.2</td>\n",
       "      <td>9269.7</td>\n",
       "      <td>9304.8</td>\n",
       "      <td>200871293.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-02</th>\n",
       "      <td>9363.5</td>\n",
       "      <td>9400.7</td>\n",
       "      <td>9322.9</td>\n",
       "      <td>9356.7</td>\n",
       "      <td>152284249.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-03</th>\n",
       "      <td>9487.8</td>\n",
       "      <td>9491.2</td>\n",
       "      <td>9436.5</td>\n",
       "      <td>9457.1</td>\n",
       "      <td>173813072.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-04</th>\n",
       "      <td>9534.1</td>\n",
       "      <td>9551.5</td>\n",
       "      <td>9472.0</td>\n",
       "      <td>9488.9</td>\n",
       "      <td>164758798.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-05</th>\n",
       "      <td>9510.3</td>\n",
       "      <td>9553.6</td>\n",
       "      <td>9476.5</td>\n",
       "      <td>9542.4</td>\n",
       "      <td>178692082.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-08</th>\n",
       "      <td>9437.7</td>\n",
       "      <td>9495.1</td>\n",
       "      <td>9425.2</td>\n",
       "      <td>9483.4</td>\n",
       "      <td>121814457.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-09</th>\n",
       "      <td>9407.8</td>\n",
       "      <td>9511.2</td>\n",
       "      <td>9388.3</td>\n",
       "      <td>9428.3</td>\n",
       "      <td>137599807.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-10</th>\n",
       "      <td>9406.5</td>\n",
       "      <td>9467.5</td>\n",
       "      <td>9395.2</td>\n",
       "      <td>9423.5</td>\n",
       "      <td>152200593.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-11</th>\n",
       "      <td>9445.4</td>\n",
       "      <td>9468.7</td>\n",
       "      <td>9372.3</td>\n",
       "      <td>9423.1</td>\n",
       "      <td>178531090.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-12</th>\n",
       "      <td>9468.5</td>\n",
       "      <td>9509.2</td>\n",
       "      <td>9390.2</td>\n",
       "      <td>9441.3</td>\n",
       "      <td>212327212.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-15</th>\n",
       "      <td>9497.1</td>\n",
       "      <td>9528.4</td>\n",
       "      <td>9468.1</td>\n",
       "      <td>9488.1</td>\n",
       "      <td>156927151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-16</th>\n",
       "      <td>9497.3</td>\n",
       "      <td>9543.7</td>\n",
       "      <td>9462.9</td>\n",
       "      <td>9492.4</td>\n",
       "      <td>140510331.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-17</th>\n",
       "      <td>9549.8</td>\n",
       "      <td>9580.6</td>\n",
       "      <td>9460.5</td>\n",
       "      <td>9486.0</td>\n",
       "      <td>193154824.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-18</th>\n",
       "      <td>9581.9</td>\n",
       "      <td>9581.9</td>\n",
       "      <td>9480.0</td>\n",
       "      <td>9553.7</td>\n",
       "      <td>165266388.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-23</th>\n",
       "      <td>9527.2</td>\n",
       "      <td>9588.2</td>\n",
       "      <td>9492.2</td>\n",
       "      <td>9586.0</td>\n",
       "      <td>159546498.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-24</th>\n",
       "      <td>9456.4</td>\n",
       "      <td>9525.2</td>\n",
       "      <td>9427.4</td>\n",
       "      <td>9506.3</td>\n",
       "      <td>157933155.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-25</th>\n",
       "      <td>9501.2</td>\n",
       "      <td>9503.8</td>\n",
       "      <td>9421.3</td>\n",
       "      <td>9450.8</td>\n",
       "      <td>147415609.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-26</th>\n",
       "      <td>9506.0</td>\n",
       "      <td>9527.4</td>\n",
       "      <td>9448.8</td>\n",
       "      <td>9512.1</td>\n",
       "      <td>153319264.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-29</th>\n",
       "      <td>9517.2</td>\n",
       "      <td>9517.2</td>\n",
       "      <td>9414.4</td>\n",
       "      <td>9435.3</td>\n",
       "      <td>146296518.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-30</th>\n",
       "      <td>9570.6</td>\n",
       "      <td>9570.6</td>\n",
       "      <td>9455.5</td>\n",
       "      <td>9497.3</td>\n",
       "      <td>186881747.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-02</th>\n",
       "      <td>9418.2</td>\n",
       "      <td>9559.7</td>\n",
       "      <td>9412.2</td>\n",
       "      <td>9545.5</td>\n",
       "      <td>205610985.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-03</th>\n",
       "      <td>9409.6</td>\n",
       "      <td>9450.7</td>\n",
       "      <td>9388.7</td>\n",
       "      <td>9427.9</td>\n",
       "      <td>126736803.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-06</th>\n",
       "      <td>9331.0</td>\n",
       "      <td>9347.2</td>\n",
       "      <td>9236.3</td>\n",
       "      <td>9259.0</td>\n",
       "      <td>121602272.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-07</th>\n",
       "      <td>9235.1</td>\n",
       "      <td>9404.6</td>\n",
       "      <td>9221.0</td>\n",
       "      <td>9332.9</td>\n",
       "      <td>199265692.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-08</th>\n",
       "      <td>9227.0</td>\n",
       "      <td>9261.2</td>\n",
       "      <td>9158.3</td>\n",
       "      <td>9233.9</td>\n",
       "      <td>185752613.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-09</th>\n",
       "      <td>9095.2</td>\n",
       "      <td>9184.8</td>\n",
       "      <td>9079.8</td>\n",
       "      <td>9162.1</td>\n",
       "      <td>166997789.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-10</th>\n",
       "      <td>9117.5</td>\n",
       "      <td>9179.4</td>\n",
       "      <td>9099.1</td>\n",
       "      <td>9149.4</td>\n",
       "      <td>147760298.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-13</th>\n",
       "      <td>9046.8</td>\n",
       "      <td>9127.6</td>\n",
       "      <td>9025.7</td>\n",
       "      <td>9127.6</td>\n",
       "      <td>147442347.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-14</th>\n",
       "      <td>9127.6</td>\n",
       "      <td>9127.6</td>\n",
       "      <td>9053.6</td>\n",
       "      <td>9078.2</td>\n",
       "      <td>139675705.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-15</th>\n",
       "      <td>9177.1</td>\n",
       "      <td>9177.1</td>\n",
       "      <td>9061.9</td>\n",
       "      <td>9143.0</td>\n",
       "      <td>143425464.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4919 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              close     high      low     open          vol\n",
       "Date                                                       \n",
       "2000-01-03  11610.0  11881.8  11574.4  11846.6   60107000.0\n",
       "2000-01-04  11206.6  11530.0  11159.8  11499.5   62539000.0\n",
       "2000-01-05  10863.1  11068.1  10824.9  11206.6   68153000.0\n",
       "2000-01-07  11102.4  11137.9  10882.7  10963.4  144207000.0\n",
       "2000-01-10  11173.3  11364.3  11120.6  11363.8  133817000.0\n",
       "2000-01-11  11012.4  11234.6  10919.9  11233.0   73650000.0\n",
       "2000-01-12  10851.8  10947.1  10821.4  10932.9  115797000.0\n",
       "2000-01-13  10931.9  11169.8  10827.6  10989.9  139485000.0\n",
       "2000-01-14  11184.0  11212.1  10976.3  10980.7   97119000.0\n",
       "2000-01-17  11285.6  11394.2  11191.5  11384.9   94058000.0\n",
       "2000-01-18  11066.3  11355.4  11032.5  11335.7   88682000.0\n",
       "2000-01-19  11120.7  11134.4  10931.6  10981.5   79358000.0\n",
       "2000-01-20  11047.6  11258.1  10970.4  11139.7   74005000.0\n",
       "2000-01-21  10969.3  11073.9  10904.4  10992.2   80696000.0\n",
       "2000-01-24  10955.0  11142.2  10908.8  11060.9   53854000.0\n",
       "2000-01-25  10863.5  10901.5  10808.8  10828.2   65985000.0\n",
       "2000-01-26  11031.4  11044.3  10923.6  10965.1   78070000.0\n",
       "2000-01-27  11206.6  11293.6  11084.2  11134.3  118800000.0\n",
       "2000-01-28  11009.3  11310.9  10987.5  11307.3   82913000.0\n",
       "2000-01-31  10835.1  10960.0  10770.5  10889.1   97823000.0\n",
       "2000-02-01  10970.0  10987.9  10840.0  10960.7   66505000.0\n",
       "2000-02-02  11195.6  11215.8  10926.7  11049.7   70471000.0\n",
       "2000-02-03  11544.1  11564.5  11208.9  11275.1   69657000.0\n",
       "2000-02-04  11580.1  11704.9  11510.7  11592.9   65574000.0\n",
       "2000-02-07  11506.8  11708.7  11469.0  11617.2   61544000.0\n",
       "2000-02-08  11834.8  11834.8  11492.0  11567.1   70379000.0\n",
       "2000-02-09  11939.5  12102.8  11851.1  11932.8   79532000.0\n",
       "2000-02-10  12020.6  12067.6  11757.8  11766.3   70339000.0\n",
       "2000-02-11  12432.6  12463.7  12133.8  12170.4   95879000.0\n",
       "2000-02-14  12458.6  12912.3  12333.5  12595.5  132608000.0\n",
       "...             ...      ...      ...      ...          ...\n",
       "2019-04-01   9341.7   9355.2   9269.7   9304.8  200871293.0\n",
       "2019-04-02   9363.5   9400.7   9322.9   9356.7  152284249.0\n",
       "2019-04-03   9487.8   9491.2   9436.5   9457.1  173813072.0\n",
       "2019-04-04   9534.1   9551.5   9472.0   9488.9  164758798.0\n",
       "2019-04-05   9510.3   9553.6   9476.5   9542.4  178692082.0\n",
       "2019-04-08   9437.7   9495.1   9425.2   9483.4  121814457.0\n",
       "2019-04-09   9407.8   9511.2   9388.3   9428.3  137599807.0\n",
       "2019-04-10   9406.5   9467.5   9395.2   9423.5  152200593.0\n",
       "2019-04-11   9445.4   9468.7   9372.3   9423.1  178531090.0\n",
       "2019-04-12   9468.5   9509.2   9390.2   9441.3  212327212.0\n",
       "2019-04-15   9497.1   9528.4   9468.1   9488.1  156927151.0\n",
       "2019-04-16   9497.3   9543.7   9462.9   9492.4  140510331.0\n",
       "2019-04-17   9549.8   9580.6   9460.5   9486.0  193154824.0\n",
       "2019-04-18   9581.9   9581.9   9480.0   9553.7  165266388.0\n",
       "2019-04-23   9527.2   9588.2   9492.2   9586.0  159546498.0\n",
       "2019-04-24   9456.4   9525.2   9427.4   9506.3  157933155.0\n",
       "2019-04-25   9501.2   9503.8   9421.3   9450.8  147415609.0\n",
       "2019-04-26   9506.0   9527.4   9448.8   9512.1  153319264.0\n",
       "2019-04-29   9517.2   9517.2   9414.4   9435.3  146296518.0\n",
       "2019-04-30   9570.6   9570.6   9455.5   9497.3  186881747.0\n",
       "2019-05-02   9418.2   9559.7   9412.2   9545.5  205610985.0\n",
       "2019-05-03   9409.6   9450.7   9388.7   9427.9  126736803.0\n",
       "2019-05-06   9331.0   9347.2   9236.3   9259.0  121602272.0\n",
       "2019-05-07   9235.1   9404.6   9221.0   9332.9  199265692.0\n",
       "2019-05-08   9227.0   9261.2   9158.3   9233.9  185752613.0\n",
       "2019-05-09   9095.2   9184.8   9079.8   9162.1  166997789.0\n",
       "2019-05-10   9117.5   9179.4   9099.1   9149.4  147760298.0\n",
       "2019-05-13   9046.8   9127.6   9025.7   9127.6  147442347.0\n",
       "2019-05-14   9127.6   9127.6   9053.6   9078.2  139675705.0\n",
       "2019-05-15   9177.1   9177.1   9061.9   9143.0  143425464.0\n",
       "\n",
       "[4919 rows x 5 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ibex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
